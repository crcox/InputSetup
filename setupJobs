#!/usr/bin/env python
from __future__ import print_function
import argparse
import json
import os
import pkg_resources
import shutil
import subprocess
import yaml
from mako.template import Template
from pycon import pycon
try:
    from hyperband import hyperband, get_random_hyperparameter_configuration
except ImportError:
    pass
try:
    import progress.bar
except ImportError:
    pass
resource_package = 'pycon';
#resource_path_dag = os.path.join('templates','subdag.mako')
#dag_template_string = pkg_resources.resource_string(resource_package, resource_path_dag)
resource_path_submit = os.path.join('templates','process.mako')
submit_template_string = pkg_resources.resource_string(resource_package, resource_path_submit)

p = argparse.ArgumentParser()
# Required Positional Arguments
p.add_argument('stub')
# Flags
p.add_argument('-l','--local_data',action='store_true')
p.add_argument('-m','--write_master',action='store_true')
# Options
p.add_argument('-d','--dag_setup',type=str,default='',help="Provide config file for submit and dag files.")
p.add_argument('-o','--offset_index',type=int,nargs=1,default=0,help='Number of the first directory to affect/create. Intended to help add jobs to an existing set.')
p.add_argument('-r','--root_dir',type=str,nargs=1,default='.',help='Directory in which job directories will be constructed.')

args = p.parse_args()

STUBYAML = args.stub
SUBMITYAML = args.dag_setup
OFFSET = args.offset_index
ROOTDIR = args.root_dir

FLAG_LOCALDATA = args.local_data
FLAG_WRITEMASTER = args.write_master

if SUBMITYAML:
    FLAG_SETUPDAG = True
    #dag_template = Template(dag_template_string)
    submit_template = Template(submit_template_string)
    with open(SUBMITYAML,'rb') as f:
        ProcessInfo = yaml.load(f)

    if not 'PRESCRIPT' in ProcessInfo:
        ProcessInfo['PRESCRIPT'] = ''

    if not 'POSTSCRIPT' in ProcessInfo:
        ProcessInfo['POSTSCRIPT'] = ''

else:
    FLAG_SETUPDAG = False

SPECIALFIELDS = ['EXPAND','URLS','COPY']
StubAsMaster = False

with open(STUBYAML,'rb') as f:
    stub = yaml.load(f)

try:
    EXPAND = pycon.utils.flatten(stub['EXPAND'])
except KeyError:
    print("Stub does not include an EXPAND list. Treating stub as master...")
    StubAsMaster = True

sharedir = os.path.join(ROOTDIR,'shared')
if not os.path.isdir(sharedir):
    os.makedirs(sharedir)

#############################################################
#                HYPERBAND initialization                   #
#############################################################
if 'HYPERBAND' in stub:
    budget = stub['HYPERBAND']['budget']
    eta = stub['HYPERBAND']['aggressiveness']
    hyperparams = stub['HYPERBAND']['hyperparameters']
    BRACKETS = hyperband(budget, eta)
    for p in hyperparams:
        distribution = stub[p]['distribution']
        dist_args = stub[p]['args']
        stub[p] = [[
            get_random_hyperparameter_configuration(distribution, dist_args)
            for i in range(s['n'][0]) ]
            for s in BRACKETS ]

    stub['BRACKETS'] = BRACKETS
    stub['EXPAND'].append(['BRACKETS'] + hyperparams)

    print("Writing stub_hb.yaml...")
    with open('stub_hb.yaml', 'w') as f:
        yaml.dump(dict((k,v) for k,v in stub.items() if k!='HYPERBAND'), f)
        #yaml.dump({k:v for k,v in stub.items() if k!='HYPERBAND'}, f)

#############################################################
#         Copy files to be shared into ./shared             #
#############################################################
# After copying, the file path is updated so that the
# executable will reference the copied version of the file. If
# FLAG_LOCALDATA is True, then the path will be set to look
# for the file in the job's current directory. This is for jobs
# that run on HTCondor, since all files are transfered into the
# job directory (directories themselves are not transfered).
if 'COPY' in stub:
    COPY_shared = [field for field in stub['COPY'] if field not in EXPAND]
else:
    COPY_shared = []

for field in COPY_shared:
    if isinstance(stub[field],list):
        SourceList = stub[field]
        for iSource,source in enumerate(SourceList):
            target = os.path.join(sharedir,os.path.basename(source))
            shutil.copyfile(source, target)
            if FLAG_LOCALDATA:
                stub[field][iSource] = os.path.basename(target)
            else:
                stub[field][iSource] = os.path.normpath(os.path.join('..',sharedir,os.path.basename(target)))
    else:
        source = stub[field]
        target = os.path.join(sharedir,os.path.basename(source))
        shutil.copyfile(source, target)
        if FLAG_LOCALDATA:
            stub[field] = os.path.basename(target)
        else:
            stub[field] = os.path.normpath(os.path.join('..',sharedir,os.path.basename(target)))

###############################################################
# Log files that all jobs will pull from SQUID in URLS_SHARED #
###############################################################
# After logging, the file path will be updated so that the
# executable will look for the file in the job's current
# directory. HTCondor transfers all files into the job directory
# and does not transfer directories, themselves.
if 'URLS' in stub:
    URLS_shared = [field for field in stub['URLS'] if field not in EXPAND]
else:
    URLS_shared = []

URLS = os.path.join(sharedir,'URLS_SHARED')
with open(URLS,'w') as f:
    for field in URLS_shared:
        if isinstance(stub[field],list):
            SourceList = stub[field]
            for iSource,source in enumerate(SourceList):
                source_stripped = pycon.utils.lstrip_pattern(source,'/squid')
                f.write(source_stripped+'\n')
                if FLAG_LOCALDATA:
                    stub[field][iSource] = os.path.basename(source)
                else:
                    stub[field][iSource] = os.path.normpath(os.path.join('..',sharedir,os.path.basename(source)))
        else:
            source = stub[field]
            source_stripped = pycon.utils.lstrip_pattern(source,'/squid')
            f.write(source_stripped+'\n')
            if FLAG_LOCALDATA:
                stub[field] = os.path.basename(source)
            else:
                stub[field] = os.path.normpath(os.path.join('..',sharedir,os.path.basename(source)))

if StubAsMaster:
    master = stub
else:
    master = pycon.expand_stub(stub)

if FLAG_WRITEMASTER:
    print("Writing master.yaml...")
    with open('master.yaml', 'w') as f:
        yaml.dump_all(master, f)

#############################################################
#              Setup individual jobs structure              #
#############################################################
njobs = len(master)
print("Composing {njobs:d} individual jobs...".format(njobs=njobs))
width = pycon.utils.ndigits(njobs-1)
JobDirs = [os.path.join(ROOTDIR, "{job:0{w}d}".format(job=i,w=width)) for i in range(njobs)]
if 'COPY' in stub:
    COPY_uniq = [field for field in stub['COPY'] if field in EXPAND]
else:
    COPY_uniq = []

if 'URLS' in stub:
    URLS_uniq = [field for field in stub['URLS'] if field in EXPAND]
else:
    URLS_uniq = []

try:
    bar = progress.bar.Bar('', max=njobs)
except NameError:
    pass
for iJob,config in zip(JobDirs,master):
    if not os.path.isdir(iJob):
        os.makedirs(iJob)
    #############################################################
    #                       Copy files                          #
    #############################################################
    for field in COPY_uniq:
        if isinstance(config[field], list):
            SourceList = config[field]
            for iSource,source in enumerate(SourceList):
                target = os.path.join(iJob,os.path.basename(source))
                shutil.copyfile(source, target)
                if FLAG_LOCALDATA:
                    config[field][iSource] = os.path.basename(target)
                else:
                    config[field][iSource] = os.path.normpath(target)
        else:
            source = config[field]
            target = os.path.join(iJob,os.path.basename(source))
            shutil.copyfile(source, target)
            if FLAG_LOCALDATA:
                config[field] = os.path.basename(target)
            else:
                config[field] = os.path.normpath(target)

    #############################################################
    #                    Write URLS files                       #
    #############################################################
    URLS = os.path.join(iJob,'URLS')
    with open(URLS,'w') as f:
        for field in URLS_uniq:
            if isinstance(config[field], list):
                SourceList = config[field]
                for iSource,source in enumerate(SourceList):
                    source_stripped = pycon.utils.lstrip_pattern(source,'/squid')
                    f.write(source_stripped +'\n')
                    if FLAG_LOCALDATA:
                        config[field][iSource] = os.path.basename(source)
                    else:
                        config[field][iSource] = os.path.normpath(source)
            else:
                source = config[field]
                source_stripped = pycon.utils.lstrip_pattern(source,'/squid')
                f.write(source_stripped +'\n')
                if FLAG_LOCALDATA:
                    config[field] = os.path.basename(source)
                else:
                    config[field] = os.path.normpath(source)

    #############################################################
    #          Setup DAG and SUBMIT files for condor            #
    #############################################################
    if FLAG_SETUPDAG:
    #    dag_text = dag_template.render(UNIQUE=iJob[2:],JOBDIR='./',SUBMITFILE=os.path.abspath(os.path.join(iJob,'process.sub')),PRESCRIPT=ProcessInfo['PRESCRIPT'],POSTSCRIPT=ProcessInfo['POSTSCRIPT'])
    #    dag_filename = os.path.join(iJob,"{j:s}.dag".format(j=iJob))
    #    with open(dag_filename,'w') as f:
    #        f.write(dag_text.strip())

        submit_text = submit_template.render(ProcessInfo=ProcessInfo,UNIQUE=iJob[2:],JOBDIR=iJob)
        submit_filename = os.path.join(iJob,"process.sub")
        with open(submit_filename,'w') as f:
            f.write(submit_text.strip())

    #############################################################
    #           Distribute params.json file to each job         #
    #############################################################
    paramfile = os.path.join(iJob,'params.json')
    if 'COPY' in config:
        del config['COPY']

    if 'URLS' in config:
        del config['URLS']

    with open(paramfile, 'w') as f:
        json.dump(config, f, sort_keys = True, indent = 4)
    try:
        bar.next()
    except NameError:
        pass

if FLAG_SETUPDAG:
    with open('./sweep.dag','w') as f:
        if os.path.isfile('./dagman.cfg'):
            f.write('CONFIG ./dagman.cfg\n')
        for iJob in JobDirs:
            submit_basename = "process.sub"
            f.write("JOB {j:s} {p:s} DIR {d:s}\n".format(j=iJob[2:],p=submit_basename,d=iJob))

try:
    bar.finish()
except NameError:
    pass

with open('.njobs','w') as f:
    f.write(str(njobs)+'\n')

print("Done.")

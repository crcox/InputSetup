#!/usr/bin/env python
from __future__ import absolute_import, division, print_function
from io import open
import argparse
import os
import sys
import pandas
import pkg_resources
import yaml
from hyperband import pick_best_hyperparameters
from mako.template import Template
from mako import exceptions
resource_package = 'pycon'

resource_path_stub = os.path.join('templates','stub.mako')
stub_template_string = pkg_resources.resource_string(resource_package, resource_path_stub)
stub_template = Template(stub_template_string)

p = argparse.ArgumentParser()
# Required Positional Arguments
p.add_argument('method', help="soslasso, iterlasso, lasso, searchlight. ToDo: searchlightrsa, nrsa")
# Flags
p.add_argument('-v','--verbose',action='store_true',help="Print parameter details into the stub file as comments.")
p.add_argument('-H','--hyperband',action='store_true',help="Tweak some parameters to be compatible with a HYPERBAND setup.")
# Options
p.add_argument('-m','--metadata',type=str,help="Path to metadata file.")
p.add_argument('-d','--data',type=str,nargs='+',help='List of paths to include in the data list.')
p.add_argument('-r','--random_permutations',type=int,nargs='+',default=[0],help="If greater than zero, generate a template for doing r random permutations. If two arguments are provided, the second specifies how many batches the permutations shuold be split in to.")
p.add_argument('-k','--kfolds',type=int,default=10,help='Set up k-fold cv for a particular k.')
p.add_argument('-t','--tuning-error',type=str,nargs='+',default=[],help="Provide path to one or more csv files containing error for multiple hyperparameter configurations. The stub will incorporate the 'best' parameters.")
p.add_argument('-s','--tuning-stub',type=str,default='',help="Provide path to the stub.yaml file that was used to specify a round of tuning. This can be used in conjunction with --tuning-error to generate a final/stub.yaml.")
p.add_argument('-c','--config-stub',type=str,default='',help="Provide path to a yaml file that will define how to generate the tuning stub.")
p.add_argument('-a','--argmax',action='store_true',help='When tuning error is provided, this toggles whether to choose the hyperparameter configuration with the maximum objective value [the default is to minimize].')
p.add_argument('-b','--by',type=str,nargs='+',default=['subject','finalholdout'],help="When tuning error is provided, this specifies which fields to group by when picking the 'best' hyperparameter configuration [the defaults are 'subject','finalholdout'].")
p.add_argument('-p','--hyperparameters',type=str,nargs='+',default=['lambda'],help="When tuning error is provided, this specifies which fields to treat as hyperparameters when picking the 'best' hyperparameter configuration [the default is 'lambda'].")
p.add_argument('-x','--objective',type=str,default='err1',help="When tuning error is provided, this specifies which field to treat as the objective value when picking the 'best' hyperparameter configuration [the default is 'err1'].")
p.add_argument('-o','--output',type=str,default='',help="A file to write output to. Output will be YAML formatted. [default is to print to stdout]")

args = p.parse_args()
X = {}
final = False
tuningstub = False
if args.tuning_error:
    final = True
    if args.tuning_stub:
        with open(args.tuning_stub,'rb') as f:
            tuningstub = yaml.load(f)

        if args.metadata:
            X['metadata'] = args.metadata
        else:
            X['metadata'] = tuningstub['metadata']

        X['bias'] = tuningstub['bias']
        X['normalize'] = tuningstub['normalize']
        X['data_var'] = tuningstub['data_var']
        X['metadata_var'] = tuningstub['metadata_var']
        X['filters'] = tuningstub['filters']
        X['cvscheme'] = tuningstub['cvscheme']
        X['target'] = tuningstub['target']
        X['target_type'] = tuningstub['target_type']
        X['sim_source'] = tuningstub['sim_source']
        X['sim_metric'] = tuningstub['sim_metric']
        X['tau'] = tuningstub['tau']
        X['subject_id_fmt'] = tuningstub['subject_id_fmt']
        X['executable'] = tuningstub['executable']
        X['wrapper'] = tuningstub['wrapper']

    df = pandas.concat([pandas.read_csv(f) for f in args.tuning_error])

    if args.method in ['soslasso','lasso']:
        df['diff'] = (df['h1']/df['nt1']) - (df['f1']/df['nd1'])

    z = pick_best_hyperparameters(df, args.by, args.hyperparameters, args.objective, args.argmax)
    # n = len(args.by + args.hyperparameters)
    # z = y.assign(hyper_min=[x[len(args.by):n] for x in z[args.objective]).drop(args.objective, 1)
    for i,x in enumerate(args.by):
        if len(args.by) > 1:
            levels = z.index.levels[i]
            if x == 'subject' and args.data:
                X[x] = [args.data[j] for j in z.index.labels[i]]
            elif x == 'subject' and tuningstub:
                X[x] = [tuningstub['data'][j] for j in z.index.labels[i]]
            else:
                X[x] = [levels[j] for j in z.index.labels[i]]
        else:
            if x == 'subject' and args.data:
                X[x] = [args.data[j] for j in z.index]
            elif x == 'subject' and tuningstub:
                X[x] = [tuningstub['data'][j] for j in z.index]
            else:
                X[x] = list(z.index)


    for i,x in enumerate(args.hyperparameters):
        X[x] = z[x].values.tolist()

if args.config_stub:
    with open(args.config_stub,'rb') as f:
        X = yaml.load(f)

try:
    stub_text = stub_template.render(HOME=os.getenv('HOME'), method=args.method,
            data=args.data, metadata=args.metadata, r=args.random_permutations,
            k=args.kfolds, hyperband=args.hyperband, verbose=args.verbose, override=X, final=final)
except:
    with open('mako_error.html', 'wb') as f:
        f.write(exceptions.html_error_template().render())
    raise

if args.output:
    with open(args.output, 'w', newline='\n') as h:
        h.write(stub_text)
else:
    sys.stdout.write(stub_text)
